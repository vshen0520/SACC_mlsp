{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fbank as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2138\n",
      "{'sinhalese1.mp3', 'nicaragua.mp3'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\py\\anaconda3\\lib\\site-packages\\librosa\\util\\decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import wave\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from python_speech_features import fbank\n",
    "from audio2numpy import open_audio\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# load raw data\n",
    "data=pd.read_csv(\"archive/speakers_all.csv\",index_col='speakerid')\n",
    "data = data.iloc[:,:8]\n",
    "data = data[data['file_missing?']==False]\n",
    "data\n",
    "\n",
    "INPUT_DIR = 'archive/recordings/recordings/'\n",
    "OUTPUT_DIR = 'results/'\n",
    "parent_list = os.listdir(INPUT_DIR)\n",
    "print(len(parent_list))\n",
    "\n",
    "print(set(data['filename']+'.mp3') - set(parent_list)) # find contries whose recordings are misssing\n",
    "\n",
    "data = data.drop(data[data['filename']=='sinhalese1'].index)\n",
    "data = data.drop(data[data['filename']=='nicaragua'].index)\n",
    "data\n",
    "\n",
    "# 4 classification\n",
    "English = data[data['native_language']=='english']\n",
    "French = data[data['native_language']=='french']\n",
    "Spanish= data[data['native_language']=='spanish']\n",
    "Arabic = data[data['native_language']=='arabic']\n",
    "dataSub = English.append(French).append(Spanish).append(Arabic)\n",
    "dataSub\n",
    "\n",
    "def get_fbank(INPUT_DIR):\n",
    "    parent_list = dataSub['filename']+ '.mp3'\n",
    "    # 2138 recordings each has 20 3990-dimension Fbanks (dim depends on duration)\n",
    "    fbank_feat = np.zeros((len(parent_list),20,3990))\n",
    "    i=0  \n",
    "    for file in parent_list[:]:\n",
    "        f_name = str(INPUT_DIR+file)\n",
    "        y, sr = librosa.load(f_name,sr=None,duration=20)   # duration 20s, uses the native sampling rate\n",
    "        fb = fbank(y,sr/2,nfft=1103,nfilt=20)[0].T\n",
    "        \n",
    "        if len(fb[1]) < 3990:\n",
    "            offset = 3990 - len(fb[1]) # padding starting point\n",
    "            fb= np.pad(fb,((0,0),(offset,0)), 'constant')\n",
    "   \n",
    "        if len(fb[1]) > 3990:\n",
    "            fb = fb[:,:3990]    # extract the first 3990 dims\n",
    "        \n",
    "        fbank_feat[i,:,:]= fb\n",
    "        i+=1\n",
    "        \n",
    "    return fbank_feat\n",
    "\n",
    "fbank_feat = get_fbank(INPUT_DIR)\n",
    "np.save(\"Results/fbank_feat4.npy\",fbank_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(906, 20, 3990)\n"
     ]
    }
   ],
   "source": [
    "X = np.load(\"Results/fbank_feat4.npy\")\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(906, 20, 3990)\n"
     ]
    }
   ],
   "source": [
    "#PCA \n",
    "from sklearn.decomposition import PCA\n",
    "proj = np.zeros(shape=(906,20,3990))\n",
    "\n",
    "for i in range(906):\n",
    "    X1 = X[i,:,:]\n",
    "    pca = PCA(whiten=True)\n",
    "    X_pca = pca.fit_transform(X1)\n",
    "    proj[i,:,:] = pca.inverse_transform(X_pca)\n",
    "\n",
    "X = proj\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(906, 4)\n"
     ]
    }
   ],
   "source": [
    "y_accent = dataSub['native_language']\n",
    "\n",
    "encoder1 = LabelEncoder()\n",
    "encoder1.fit(y_accent)\n",
    "y_accent_ = encoder1.transform(y_accent)\n",
    "y_accent_ = to_categorical(np.array(y_accent_),dtype='float32')\n",
    "print(np.shape(y_accent_))  #4 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (724, 20, 3990, 1) (724, 4)\n",
      "Test set: (182, 20, 3990, 1) (182, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_accent_train, y_accent_test = train_test_split(X, y_accent_, test_size=0.2, random_state=10)\n",
    "\n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "\n",
    "X_train = (X_train - mean)/std\n",
    "X_test = (X_test - mean)/std\n",
    "X_train= np.expand_dims(X_train, axis=3)\n",
    "X_test= np.expand_dims(X_test, axis=3)\n",
    "\n",
    "#X_train, X_val, y_accent_train, y_accent_val = train_test_split(X_train, y_accent_train, test_size=0.15, random_state=10)\n",
    "#X_val = (X_val - mean)/std\n",
    "#X_val= np.expand_dims(X_val, axis=3)\n",
    "\n",
    "print ('Train set:', X_train.shape,  y_accent_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_accent_test.shape)\n",
    "#print ('validation set:', X_val.shape,  y_accent_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (724, 79800) (724,)\n",
      "Test set: (182, 79800) (182,)\n"
     ]
    }
   ],
   "source": [
    "newX_train = X_train.reshape(724, 79800)\n",
    "newX_test = X_test.reshape(182, 79800)\n",
    "\n",
    "y_label_train = y_accent_train.argmax(axis=1)\n",
    "y_label_test =  y_accent_test.argmax(axis=1)\n",
    "\n",
    "print ('Train set:', newX_train.shape,  y_label_train.shape)\n",
    "print ('Test set:', newX_test.shape,  y_label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6593406593406593\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling for Classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(newX_train)\n",
    "\n",
    "newX_train = scaler.transform(newX_train)\n",
    "newX_test = scaler.transform(newX_test)\n",
    "\n",
    "# Training and Predicting for Classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier()\n",
    "classifier.fit(newX_train, y_label_train)\n",
    "y_pred = classifier.predict(newX_test)\n",
    "\n",
    "# print accuracy\n",
    "acc =  classifier.score(newX_test, y_label_test)\n",
    "print(acc) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
